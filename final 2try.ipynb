{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56514cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "window=Tk()\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords ,wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "import networkx as nx\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b5c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "allurls=[]\n",
    "search_index=[]\n",
    "bool_index=[]\n",
    "inverted_index={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c33c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bool_index(word):\n",
    "    docs=allurls\n",
    "    bool_index = np.zeros(len(docs), dtype=int)\n",
    "    \n",
    "    for j in range(len(docs)):\n",
    "        \n",
    "        if word in get_terms(docs[j]):\n",
    "            bool_index[j] = 1\n",
    "    return bool_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dc92eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageRank():\n",
    "    def __init__(self, node_count, graph, iters, dr):\n",
    "        self.node_count = node_count\n",
    "        self.graph = graph\n",
    "        self.page_rank = np.zeros(node_count)\n",
    "        self.itres = iters\n",
    "        self.dr = dr\n",
    "        \n",
    "    def calc_outgoing_links(self, node):\n",
    "        outgoing_links = 0\n",
    "        for i in range(self.node_count):\n",
    "            if self.graph[i] == 1:\n",
    "                outgoing_links += 1\n",
    "        return outgoing_links\n",
    "    \n",
    "    def calc_pr_for_single_node(self,node):\n",
    "        page_rank = 0\n",
    "        for o in range(self.node_count):\n",
    "            if self.graph[o] == 1:\n",
    "                page_rank += (1 - self.dr) + self.dr * self.page_rank[o] / self.calc_outgoing_links(o)\n",
    "        return page_rank\n",
    "    \n",
    "    def calc_full_page_rank(self):\n",
    "        init_page_rank = 1 / self.node_count\n",
    "        for i in range(self.node_count):\n",
    "            self.page_rank[i] = init_page_rank  \n",
    "        for j in range(self.itres):\n",
    "            temp_page_rank = np.zeros(self.node_count)\n",
    "            for i in range(self.node_count):\n",
    "                temp_page_rank[i] = self.calc_pr_for_single_node(i)\n",
    "            lb.insert(END,f\"Page Rank after {j} iteration:\")\n",
    "            for i in range(self.node_count):\n",
    "                lb.insert(END,f\"Page rank for node {i} is {temp_page_rank[i]}\")\n",
    "            lb.insert(END,\"---------------------------\")\n",
    "            self.page_rank = temp_page_rank\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36bd2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pg_search():\n",
    "    tokens1 = word_tokenize(tSh.get())\n",
    "    for i in tokens1:\n",
    "        pr = PageRank(5, build_bool_index(i), 20, 0.85)\n",
    "        pr.calc_full_page_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aedd179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index(docs):\n",
    "    \n",
    "    for i in range (10):\n",
    "        terms= get_terms(docs[i])\n",
    "        for term in terms:\n",
    "            if term in inverted_index.keys():\n",
    "                if i not in inverted_index[term]:\n",
    "                    inverted_index[term].append(i)\n",
    "            else:\n",
    "                inverted_index.update({term:[i]})\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5d20d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terms(doc):\n",
    "        page_content = requests.get(doc).text\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        results = soup.get_text()\n",
    "        stopset = set(stopwords.words('english'))\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = wordpunct_tokenize(results)\n",
    "        clean = [token.lower() for token in tokens if token.lower() not in stopset ]\n",
    "        final = [stemmer.stem(word) for word in clean]\n",
    "        return set(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c1acb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_search(query):\n",
    "    query=query.lower()\n",
    "    stemmer = PorterStemmer()\n",
    "    query=stemmer.stem(query)\n",
    "    for i in inverted_index[query]:\n",
    "        search_index=allurls[i] \n",
    "        \n",
    "        \n",
    "   \n",
    "    return inverted_index[query]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfe20618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class crawler:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.to_visit = list()\n",
    "        self.visited = set()\n",
    "\n",
    "    def fetch(self, url):\n",
    "        print('now fetching.. ' , url)\n",
    "        \n",
    "        try:\n",
    "            res = requests.get(url).content\n",
    "\n",
    "            return res\n",
    "        except:\n",
    "            return\n",
    "\n",
    "    def get_current_url(self):\n",
    "        res = self.to_visit.pop()\n",
    "\n",
    "        while res in self.visited:\n",
    "            print('already visited',res)\n",
    "            res = self.to_visit.pop()\n",
    "\n",
    "        return res\n",
    "\n",
    "    def get_links(self , content ):\n",
    "        urls = re.findall('<a href=\"([^\"]+)\">', str(content))\n",
    "        print('urls are', urls)\n",
    "        for url in urls :\n",
    "            if url != '#':\n",
    "                allurls.append(url)\n",
    "                pattern = re.compile('http?')\n",
    "\n",
    "                if pattern.match(url):\n",
    "                    self.to_visit.append(url)\n",
    "\n",
    "    def  crawl(self, url, depth= 5):\n",
    "        self.to_visit.append(url)\n",
    "        while len(self.visited) <= depth:\n",
    "            \n",
    "            \n",
    "            current_url = self.get_current_url()\n",
    "            \n",
    "\n",
    "            content = self.fetch(current_url)\n",
    "\n",
    "            self.visited.add(current_url)\n",
    "            self.get_links(content)\n",
    "\n",
    "        print('not visted',self.to_visit)\n",
    "        print('visited',self.visited)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64f8241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start() :\n",
    "    inverted_index=build_inverted_index(allurls)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2508d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(w,doc):\n",
    "    page_content = requests.get(doc).text\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    results = soup.get_text()\n",
    "    words = word_tokenize(results)\n",
    "    freq = [1  for s in words if s == w]\n",
    "    \n",
    "    return len(freq)/ len(words)\n",
    "\n",
    "\n",
    "def compute_idf(w,search_index):\n",
    "    sents=len(allurls)\n",
    "    return np.log(sents/ len(search_index))\n",
    "\n",
    "\n",
    "def compute_tfidf(word,search_index):\n",
    "    for i in search_index:\n",
    "        tf=compute_tf(word,allurls[i])\n",
    "        \n",
    "    return compute_idf(word,search_index)*tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bef3f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_search():\n",
    "    tokens1 = word_tokenize(tSh.get())\n",
    "    for i in tokens1:\n",
    "       \n",
    "        search_index=inverted_search(i)\n",
    "        tf=compute_tfidf(i,search_index)\n",
    "        for j in search_index:\n",
    "            \n",
    "            lb.insert(END,{tf,i,allurls[j]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec78f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Add directed edges in graph\n",
    "def add_edges(g, pr):\n",
    "\tfor each in g.nodes():\n",
    "\t\tfor each1 in g.nodes():\n",
    "\t\t\tif (each != each1):\n",
    "\t\t\t\tra = random.random()\n",
    "\t\t\t\tif (ra < pr):\n",
    "\t\t\t\t\tg.add_edge(each, each1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcontinue\n",
    "\treturn g\n",
    "\n",
    "# Sort the nodes\n",
    "def nodes_sorted(g, points):\n",
    "\tt = np.array(points)\n",
    "\tt = np.argsort(-t)\n",
    "\treturn t\n",
    "\n",
    "# Distribute points randomly in a graph\n",
    "def random_Walk(g):\n",
    "\trwp = [0 for i in range(g.number_of_nodes())]\n",
    "\tnodes = list(g.nodes())\n",
    "\tr = random.choice(nodes)\n",
    "\trwp[r] += 1\n",
    "\tneigh = list(g.out_edges(r))\n",
    "\tz = 0\n",
    "\t\n",
    "\twhile (z != 10000):\n",
    "\t\tif (len(neigh) == 0):\n",
    "\t\t\tfocus = random.choice(nodes)\n",
    "\t\telse:\n",
    "\t\t\tr1 = random.choice(neigh)\n",
    "\t\t\tfocus = r1[1]\n",
    "\t\trwp[focus] += 1\n",
    "\t\tneigh = list(g.out_edges(focus))\n",
    "\t\tz += 1\n",
    "\treturn rwp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "444da83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thrd_search():\n",
    "    tokens1 = word_tokenize(tSh.get())\n",
    "    for i in tokens1:       \n",
    "        search_index=inverted_search(i)\n",
    "        # Main\n",
    "    # 1. Create a directed graph with N nodes\n",
    "        g = nx.DiGraph()\n",
    "\n",
    "        N = len(search_index)\n",
    "        g.add_nodes_from(range(N))\n",
    "\n",
    "        # 2. Add directed edges in graph\n",
    "        g = add_edges(g, 0.4)\n",
    "\n",
    "        # 3. perform a random walk\n",
    "        points = random_Walk(g)\n",
    "\n",
    "        # 4. Get nodes rank according to their random walk points\n",
    "        sorted_by_points = nodes_sorted(g, points)\n",
    "        lb.insert(END,\"PageRank using Random Walk Method\")\n",
    "        for i in sorted_by_points:\n",
    "            lb.insert(END,allurls[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fee76e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-34-d8dc026f0622>\", line 7, in <lambda>\n",
      "    thrdbtn=Button(window, text=\"3rd method\",command = lambda:[thrd_search(), lb.delete(0,END)])\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\tkinter\\__init__.py\", line 3602, in delete\n",
      "    self.tk.call(self._w, 'delete', index1, index2)\n",
      "_tkinter.TclError: bad text index \"0\"\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\tkinter\\__init__.py\", line 1892, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-34-d8dc026f0622>\", line 7, in <lambda>\n",
      "    thrdbtn=Button(window, text=\"3rd method\",command = lambda:[thrd_search(), lb.delete(0,END)])\n",
      "  File \"C:\\Users\\User\\anaconda3\\lib\\tkinter\\__init__.py\", line 3602, in delete\n",
      "    self.tk.call(self._w, 'delete', index1, index2)\n",
      "_tkinter.TclError: bad text index \"0\"\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "window=Tk()\n",
    "# add widgets here\n",
    "crawlbtn=Button(window, text=\"crawler\",command = lambda:[crawler().crawl(tURl.get()),start()])\n",
    "tfIdfbtn=Button(window, text=\"tf idf\",command = lambda:[df_search(), lb.delete(0,END)])\n",
    "pgrbtn=Button(window, text=\"page rank\",command = lambda:[pg_search(), lb.delete(0,END)])\n",
    "thrdbtn=Button(window, text=\"3rd method\",command = lambda:[thrd_search(), lb.delete(0,END)])\n",
    "\n",
    "crawlbtn.place(x=90, y=200)\n",
    "tfIdfbtn.place(x=60, y=250)\n",
    "pgrbtn.place(x=120, y=250)\n",
    "thrdbtn.place(x=80, y=300)\n",
    "\n",
    "\n",
    "lblurl=Label(window, text=\"enter your URL ..\", fg='red', font=(\"Helvetica\", 16))\n",
    "lblurl.place(x=50, y=50)\n",
    "tURl=Entry(window, text=\"\")\n",
    "\n",
    "tURl.place(x=55, y=80)\n",
    "\n",
    "lblsh=Label(window, text=\"enter your search ..\", fg='red', font=(\"Helvetica\", 16))\n",
    "lblsh.place(x=50, y=110)\n",
    "\n",
    "tSh=Entry(window, text=\"\")\n",
    "tSh.place(x=55, y=150)\n",
    "lb=Text(window,width=110, height=40)\n",
    "lb.place(x=300, y=50)\n",
    "\n",
    "window.title('Hello Python')\n",
    "window.geometry(\"1200x700+10+20\")\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dce5e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
